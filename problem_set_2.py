# -*- coding: utf-8 -*-
"""pipeline-fraud-detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZYEn4uYuDHa5FSvz6QAcSYYg0qSe8oJc
"""

!pip install mlflow

!pip install kaggle

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

import pandas as pd
import mlflow

!kaggle competitions download -c ieee-fraud-detection



from google.colab import drive
drive.mount('/content/drive')

import os
import mlflow

mlflow_dir = "/content/drive/MyDrive/mlflow_fraud"
os.makedirs(mlflow_dir, exist_ok=True)

mlflow.set_tracking_uri(f"sqlite:///{mlflow_dir}/mlflow.db")
mlflow.set_experiment("fraud-detection-experiment")

"""# Loading Data"""

!unzip -q ieee-fraud-detection.zip -d ieee-fraud-detection

train_identify_df=pd.read_csv('ieee-fraud-detection/train_identity.csv')
test_identify_df=pd.read_csv('ieee-fraud-detection/test_identity.csv')
train_transaction_df=pd.read_csv('ieee-fraud-detection/train_transaction.csv')
test_transaction_df=pd.read_csv('ieee-fraud-detection/test_transaction.csv')

train_identify_df.isna().sum()

train_identify_df.info()

train_transaction_df.info()

train= pd.merge(train_transaction_df,train_identify_df, on='TransactionID', how='left')

print(test_identify_df.info())
print(test_transaction_df.info())

test=pd.merge(test_transaction_df,test_identify_df, on='TransactionID',how='left')

train.info()

test.info()

train

print(f'{train.shape[0]} rows and {train.shape[1]} columns')
print(f'{test.shape[0]} rows and {test.shape[1]} columns')

train.head()

"""# EDA"""

missing_ratio = train.isnull().sum() / len(train)

print(missing_ratio)

drop_cols = missing_ratio[missing_ratio > 0.9].index

drop_cols

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt

# %matplotlib inline

print('there are missing values:',train.isnull().any().sum())

train.head()

train['isFraud'].isnull().sum()

plt.hist(train['id_01'], bins=77);
plt.title('Distribution of id_01 variable');

plt.hist(train['TransactionDT'], label='train');
plt.hist(test['TransactionDT'], label='test');
plt.legend();
plt.title('Distribution of transactiond dates');

threshold = 0.9  # 90% missing threshold

# Buat X dari train tanpa target dan kolom waktu
X = train.drop(columns=['isFraud', 'TransactionDT', 'TransactionID'], errors='ignore')

# Hitung rasio missing values
missing_ratio = X.isnull().sum() / len(X)

# Pilih kolom dengan missing ratio lebih besar dari threshold
drop_cols = missing_ratio[missing_ratio > threshold].index

# Print kolom yang akan dihapus
print(f"ðŸ›‘ Kolom yang dihapus ({len(drop_cols)}): {list(drop_cols)}")

# Hapus kolom dari train dan test (gunakan errors='ignore' untuk test)
train.drop(columns=drop_cols, axis=1, inplace=True)
test.drop(columns=drop_cols, axis=1, inplace=True, errors='ignore')

# Print jumlah kolom setelah penghapusan
print(f"âœ… Sisa kolom setelah penghapusan di train: {train.shape[1]}")
print(f"âœ… Sisa kolom setelah penghapusan di test: {test.shape[1]}")

X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)
y = train.sort_values('TransactionDT')['isFraud']
#X_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)
X_test = test.drop(['TransactionDT', 'TransactionID'], axis=1)
del train
test = test[["TransactionDT", 'TransactionID']]

import numpy as np
def clean_inf_nan(df):
    return df.replace([np.inf, -np.inf], np.nan)

# Cleaning infinite values to NaN
X = clean_inf_nan(X)
X_test = clean_inf_nan(X_test )

import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score

n_fold = 5
folds = TimeSeriesSplit(n_splits=n_fold)
folds = KFold(n_splits=5)

params = {'num_leaves': 256,
          'min_child_samples': 79,
          'objective': 'binary',
          'max_depth': 13,
          'learning_rate': 0.03,
          "boosting_type": "gbdt",
          "subsample_freq": 3,
          "subsample": 0.9,
          "bagging_seed": 11,
          "metric": 'auc',
          "verbosity": -1,
          'reg_alpha': 0.3,
          'reg_lambda': 0.3,
          'colsample_bytree': 0.9,
          #'categorical_feature': cat_cols
         }
result_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,
                                                      verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)

"""# GOOGLE COLAB DAN LAPTOP SAYA TIDAK KUAT UNTUK PROSES DATA INI KARENA SANGAT BESAR, SUDAH MELAKUKAN BERULANG KALI NAMUN TETAP CRASH SEHINGGA TIDAK BISA MELANJUTKAN PROSES SAMPAI TRAINING MODEL"""

